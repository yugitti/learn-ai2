{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "custom_style = {'axes.labelcolor': 'white',\n",
    "                'xtick.color': 'white',\n",
    "                'ytick.color': 'white'}\n",
    "sns.set_style(\"darkgrid\", rc=custom_style)\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"font.size\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PytorchでMNISTを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考サイト\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #畳み込み層を定義する\n",
    "        #引数は順番に、サンプル数、チャネル数、フィルタのサイズ\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        #フィルタのサイズは正方形であればタプルではなく整数でも可（8行目と10行目は同じ意味）\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        #全結合層を定義する\n",
    "        #fc1の第一引数は、チャネル数*最後のプーリング層の出力のマップのサイズ=特徴量の数\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #入力→畳み込み層1→活性化関数(ReLU)→プーリング層1(2*2)→出力\n",
    "        # input 28 x 28 x 1\n",
    "        # conv1 28 x 28 x 1 -> 24 x 24 x 10\n",
    "        # max_pool(kernel2) 12 x 12 x 10\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2,2) )\n",
    "        \n",
    "        #入力→畳み込み層2→活性化関数(ReLU)→プーリング層2(2*2)→出力\n",
    "        # conv2 12 x 12 x 10 -> 8 x 8 x 20\n",
    "        # max_pool(kernel2) -> 4 x 4 x 20\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv2_drop(x)\n",
    "        # output layer\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        # self.num_flat_featuresで特徴量の数を算出\n",
    "        # flatten 4 x 4 x 20 = 320\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        #Conv2dは入力を4階のテンソルとして保持する(サンプル数*チャネル数*縦の長さ*横の長さ)\n",
    "        #よって、特徴量の数を数える時は[1:]でスライスしたものを用いる\n",
    "        size = x.size()[1:] ## all dimensions except the batch dimension\n",
    "        #特徴量の数=チャネル数*縦の長さ*横の長さを計算する\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger(object):\n",
    "    \n",
    "    def __init__(self, out):\n",
    "        try:\n",
    "            os.makedirs(out)\n",
    "        except OSError:\n",
    "            pass\n",
    "        self.file = open(os.path.join(out, 'log'), 'w')\n",
    "        self.logs = []\n",
    "        \n",
    "    def write(self, log):\n",
    "        ## write log\n",
    "        tqdm.write(log)\n",
    "        tqdm.write(log, file=self.file)\n",
    "        self.logs.append(log)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        ## returns the state of the loggers\n",
    "        return {'logs': self.logs}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        ## load the logger state\n",
    "        self.logs = state_dict['logs']\n",
    "        #write logs\n",
    "        tqdm.write(self.logs[-1])\n",
    "        for log in self.logs:\n",
    "            tqdm.write(log, file=self.file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(net, optimizer, epoch, logger, out):\n",
    "    filename = os.path.join(out, 'epoch-{}'.format(epoch))\n",
    "    torch.save({'epoch': epoch + 1, 'logger': logger.state_dict()}, filename + '.iter')\n",
    "    torch.save(net.state_dict(), filename + 'model')\n",
    "    torch.save(optimizer.state_dict(), filename + 'state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch, log_interval, logger):\n",
    "    model.train()\n",
    "    for batch_id, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_id % log_interval == 0:\n",
    "            log = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_id * len(data), len(train_loader.dataset),\n",
    "                100. * batch_id / len(train_loader), loss.item())\n",
    "            logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, logger):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    log = '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, total, 100. * correct / total)\n",
    "    logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataSet(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "\n",
    "        self.image_dataframe = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.image_dataframe.loc[idx, 'img'])\n",
    "#         image = io.imread(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('L')\n",
    "        label = self.image_dataframe.loc[idx, 'label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 100\n",
    "    test_bach_size = 100\n",
    "    epochs = 30\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = False\n",
    "    seed = 123\n",
    "    log_interval = 10\n",
    "    out_dir = './result'\n",
    "    train_csv = '../data/mnist/train_big.csv'\n",
    "    test_csv = '../data/mnist/test_big.csv'\n",
    "    train_root_dir = '../data/mnist/train'\n",
    "    test_root_dir = '../data/mnist/test'\n",
    "    test_interval = 1\n",
    "    resume_interval = 1\n",
    "    \n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()    \n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda:1' if use_cuda else 'cpu')\n",
    "    print(device)\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))])\n",
    "    \n",
    "#     trainset = datasets.MNIST(\n",
    "#         root = './data', train=True, download=True,transform=transform\n",
    "#     )\n",
    "    trainset = MNISTDataSet(train_csv, train_root_dir, transform)\n",
    "#     trainset = datasets.ImageFolder(train_root_dir, transform)\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "#     testset = datasets.MNIST(\n",
    "#         root = './data', train=False, download=True,transform=transform\n",
    "#     )\n",
    "    testset = MNISTDataSet(test_csv, test_root_dir, transform)\n",
    "#     testset = datasets.ImageFolder(test_root_dir, transform)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=test_bach_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logger = TrainLogger(out_dir)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(net, device, trainloader, criterion, optimizer, epoch, log_interval, logger)\n",
    "        if epoch % test_interval == 0:\n",
    "            test(net, device, testloader, criterion, logger)\n",
    "        if epoch % resume_interval == 0:\n",
    "            checkpoint(net, optimizer, epoch, logger, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "[transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))])\n",
    "trainset = datasets.MNIST(root = '../data', train=True, download=True,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = it.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(25).reshape(5,5)\n",
    "a = torch.Tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: only one dimension can be inferred at /pytorch/aten/src/TH/THStorage.cpp:71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-2688bfa28296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: only one dimension can be inferred at /pytorch/aten/src/TH/THStorage.cpp:71"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10\n",
    "np.arange(0, m+1,m // 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1st loop:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  10%|█         | 1/10 [00:02<00:22,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  20%|██        | 2/10 [00:05<00:20,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  30%|███       | 3/10 [00:07<00:17,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  40%|████      | 4/10 [00:10<00:15,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  50%|█████     | 5/10 [00:12<00:12,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  2.00it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  60%|██████    | 6/10 [00:15<00:10,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.98it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  70%|███████   | 7/10 [00:17<00:07,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.98it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  80%|████████  | 8/10 [00:20<00:05,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.99it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop:  90%|█████████ | 9/10 [00:22<00:02,  2.53s/it]\n",
      "2nd loop:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "2nd loop:  20%|██        | 1/5 [00:00<00:02,  1.99it/s]\u001b[A\n",
      "2nd loop:  40%|████      | 2/5 [00:01<00:01,  1.98it/s]\u001b[A\n",
      "2nd loop:  60%|██████    | 3/5 [00:01<00:01,  1.99it/s]\u001b[A\n",
      "2nd loop:  80%|████████  | 4/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "2nd loop: 100%|██████████| 5/5 [00:02<00:00,  1.98it/s]\u001b[A\n",
      "1st loop: 100%|██████████| 10/10 [00:25<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "s = [s for s in range(10)]\n",
    "for i in tqdm(s, desc='1st loop'):\n",
    "    for j in trange(5, desc='2nd loop'):\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.99it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.write('TEST')\n",
    "for j in range(3):\n",
    "#     print('{}回目'.format(j))\n",
    "    for i in tqdm(range(3)):\n",
    "        sleep(0.5)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bar desc (file 12):   7%|▋         | 7/100 [00:00<00:02, 37.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bar desc (file 99): 100%|██████████| 100/100 [00:01<00:00, 63.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm.write('HELLO')\n",
    "t = trange(100, desc='Bar desc', leave=True)\n",
    "for i in t:\n",
    "    t.set_description(\"Bar desc (file %i)\" % i)\n",
    "    t.refresh() # to show immediately the update\n",
    "    sleep(0.01)\n",
    "print('HELLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 100\n",
      "HELLO\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    " \n",
    "def progress(p, l):\n",
    "    sys.stdout.write(\"\\r%d / 100\" %(int(p * 100 / (l - 1))))\n",
    "    sys.stdout.flush()\n",
    "     \n",
    "length = 100\n",
    " \n",
    "for i in range(length):\n",
    "    progress(i, length)\n",
    "     \n",
    "    #重い処理のはじまり\n",
    "    time.sleep(0.01)\n",
    "    #重い処理の終わり\n",
    "print('\\nHELLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "for i,d in enumerate(a, start=1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "HELLO\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "tqdm.write('test')\n",
    "print('HELLO')\n",
    "tqdm.write('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "pprint('TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
