{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import easydict\n",
    "\n",
    "%matplotlib inline\n",
    "custom_style = {'axes.labelcolor': 'white',\n",
    "                'xtick.color': 'white',\n",
    "                'ytick.color': 'white'}\n",
    "sns.set_style(\"darkgrid\", rc=custom_style)\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"font.size\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PytorchでMNISTを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考サイト\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--label_batch_size', type=int, default=32, metavar='N',\n",
    "#                         help='input batch size for training (default: 32)')\n",
    "#     parser.add_argument('--unlabel_batch_size', type=int, default=128, metavar='N',\n",
    "#                         help='input batch size for training (default: 32)')\n",
    "#     parser.add_argument('--test_batch_size', type=int, default=1000, metavar='N',\n",
    "#                         help='input batch size for testing (default: 1000)')\n",
    "#     parser.add_argument('--epoch', type=int, default=10, metavar='N',\n",
    "#                         help='number of iterations to train (default: 10)')\n",
    "#     parser.add_argument('--iters', type=int, default=400, metavar='N',\n",
    "#                         help='number of iterations to train (default: 10000)')\n",
    "#     parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                         help='learning rate (default: 0.01)')\n",
    "#     parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "#                         help='SGD momentum (default: 0.9)')\n",
    "#     parser.add_argument('--alpha', type=float, default=1.0, metavar='ALPHA',\n",
    "#                         help='regularization coefficient (default: 0.01)')\n",
    "#     parser.add_argument('--xi', type=float, default=10.0, metavar='XI',\n",
    "#                         help='hyperparameter of VAT (default: 0.1)')\n",
    "#     parser.add_argument('--eps', type=float, default=1.0, metavar='EPS',\n",
    "#                         help='hyperparameter of VAT (default: 1.0)')\n",
    "#     parser.add_argument('--ip', type=int, default=1, metavar='IP',\n",
    "#                         help='hyperparameter of VAT (default: 1)')\n",
    "#     parser.add_argument('--workers', type=int, default=8, metavar='W',\n",
    "#                         help='number of CPU')\n",
    "#     parser.add_argument('--seed', type=int, default=123, metavar='S',\n",
    "#                         help='random seed (default: 1)')\n",
    "#     parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "#                         help='how many batches to wait before logging training status')\n",
    "#     parser.add_argument('--gpu', type=int, default=1, metavar='W',\n",
    "#                         help='number of CPU')\n",
    "#     args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 1200)\n",
    "#         self.fc2_1 = nn.Linear(1200, 1200)\n",
    "        self.fc2 = nn.Linear(1200, 600)\n",
    "#         self.fc2_1 = nn.Linear(600, 300)\n",
    "#         self.fc2_2 = nn.Linear(300, 150)\n",
    "        self.fc3 = nn.Linear(600, 10)\n",
    "#         self.fc3 = nn.Linear(600, 10)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(1200)\n",
    "        self.bn2 = nn.BatchNorm1d(600)\n",
    "        self.bn2_1 = nn.BatchNorm1d(300)\n",
    "        self.bn2_2 = nn.BatchNorm1d(150)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn2(x)\n",
    "#         x = self.fc2_1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.bn2_1(x)\n",
    "#         x = self.fc2_2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.bn2_2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #畳み込み層を定義する\n",
    "        #引数は順番に、サンプル数、チャネル数、フィルタのサイズ\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        #フィルタのサイズは正方形であればタプルではなく整数でも可（8行目と10行目は同じ意味）\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        #全結合層を定義する\n",
    "        #fc1の第一引数は、チャネル数*最後のプーリング層の出力のマップのサイズ=特徴量の数\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #入力→畳み込み層1→活性化関数(ReLU)→プーリング層1(2*2)→出力\n",
    "        # input 28 x 28 x 1\n",
    "        # conv1 28 x 28 x 1 -> 24 x 24 x 10\n",
    "        # max_pool(kernel2) 12 x 12 x 10\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2,2) )\n",
    "        \n",
    "        #入力→畳み込み層2→活性化関数(ReLU)→プーリング層2(2*2)→出力\n",
    "        # conv2 12 x 12 x 10 -> 8 x 8 x 20\n",
    "        # max_pool(kernel2) -> 4 x 4 x 20\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv2_drop(x)\n",
    "        # output layer\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        # self.num_flat_featuresで特徴量の数を算出\n",
    "        # flatten 4 x 4 x 20 = 320\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        #Conv2dは入力を4階のテンソルとして保持する(サンプル数*チャネル数*縦の長さ*横の長さ)\n",
    "        #よって、特徴量の数を数える時は[1:]でスライスしたものを用いる\n",
    "        size = x.size()[1:] ## all dimensions except the batch dimension\n",
    "        #特徴量の数=チャネル数*縦の長さ*横の長さを計算する\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger(object):\n",
    "    \n",
    "    def __init__(self, out):\n",
    "        try:\n",
    "            os.makedirs(out)\n",
    "        except OSError:\n",
    "            pass\n",
    "        self.file = open(os.path.join(out, 'log'), 'w')\n",
    "        self.logs = []\n",
    "        \n",
    "    def write(self, log):\n",
    "        ## write log\n",
    "        tqdm.write(log)\n",
    "        tqdm.write(log, file=self.file)\n",
    "        self.logs.append(log)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        ## returns the state of the loggers\n",
    "        return {'logs': self.logs}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        ## load the logger state\n",
    "        self.logs = state_dict['logs']\n",
    "        #write logs\n",
    "        tqdm.write(self.logs[-1])\n",
    "        for log in self.logs:\n",
    "            tqdm.write(log, file=self.file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(net, optimizer, epoch, logger, out):\n",
    "    filename = os.path.join(out, 'epoch-{}'.format(epoch))\n",
    "    torch.save({'epoch': epoch + 1, 'logger': logger.state_dict()}, filename + '.iter')\n",
    "    torch.save(net.state_dict(), filename + 'model')\n",
    "    torch.save(optimizer.state_dict(), filename + 'state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteSampler(sampler.Sampler):\n",
    "\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            order = np.random.permutation(self.num_samples)\n",
    "            for i in range(self.num_samples):\n",
    "                yield order[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataSet(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "\n",
    "        self.image_dataframe = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.image_dataframe.loc[idx, 'img'])\n",
    "#         image = io.imread(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('L')\n",
    "        label = self.image_dataframe.loc[idx, 'label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(d):\n",
    "    ## 画像の場合、batch x (WxHxC) x 1 x 1\n",
    "    ## この処理の意図は最終的にバッチごとにnormを計算するため\n",
    "    d_reshape = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "#     d_norm = torch.norm(d_reshape, dim=1, keepdim=True) + 1e-8\n",
    "    \n",
    "    d_norm = torch.sqrt(torch.sum(d_reshape**2, dim=1, keepdim=True))\n",
    "    \n",
    "    return d / d_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div(log_probs, probs):\n",
    "\n",
    "#     qlogq = (probs * torch.log(probs)).sum()\n",
    "#     qlogp = (probs * log_probs).sum()\n",
    "#     return  (qlogq - qlogp).sum()\n",
    "        \n",
    "    return F.kl_div(log_probs, probs, reduction='sum')\n",
    "\n",
    "#     return F.kl_div(log_probs, probs, reduction='elementwise_mean')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def _disable_tracking_bn_stats(model):\n",
    "\n",
    "    def switch_attr(m):\n",
    "        if hasattr(m, 'track_running_stats'):\n",
    "            m.track_running_stats ^= True\n",
    "            \n",
    "    model.apply(switch_attr)\n",
    "    yield\n",
    "    model.apply(switch_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAT(nn.Module):\n",
    "    def __init__(self, xi=10.0, eps=1.0, ip=1):\n",
    "        \n",
    "        super(VAT, self).__init__()\n",
    "        self.xi = xi\n",
    "        self.eps = eps\n",
    "        self.ip = ip\n",
    "    \n",
    "    def forward(self, model, x):\n",
    "        \n",
    "        ## ラベル無しデータをネットワークに通し、predictを得る\n",
    "        ## VATのLossのbackpropagationでは、このmodel計算は含めないため\n",
    "        ## no_gradでwrapする\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            pred = F.softmax(out, dim=1)\n",
    "        \n",
    "        ## 累積法を用いてVadvを計算する\n",
    "        \n",
    "        d = torch.rand(x.shape).sub(0.5).to(x.device) ## ゼロを中心に乱数による初期値\n",
    "        d = l2_normalize(d)\n",
    "        \n",
    "        with _disable_tracking_bn_stats(model):\n",
    "            for _ in range(self.ip):\n",
    "                d.requires_grad_()\n",
    "                pred_hat = model(x + self.xi * d)\n",
    "                adv_dist = kl_div(F.log_softmax(pred_hat, dim=1), pred)\n",
    "                \n",
    "                adv_dist.backward()\n",
    "                d = l2_normalize(d.grad)\n",
    "                model.zero_grad()\n",
    "\n",
    "        ## LDSを計算する\n",
    "        r_adv = d * self.eps\n",
    "        pred_hat = model(x + r_adv)\n",
    "        lds = kl_div(F.log_softmax(pred_hat, dim=1), pred)\n",
    "        \n",
    "        return lds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "an = np.random.rand(30).reshape(3,10)\n",
    "an = an / (an.mean(axis=1).reshape(3,1))\n",
    "an = torch.from_numpy(an)\n",
    "bn = np.random.rand(30).reshape(3,10)\n",
    "bn = bn / (bn.mean(axis=1).reshape(3,1))\n",
    "bn = torch.from_numpy(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3286, dtype=torch.float64)\n",
      "tensor(9.8591, dtype=torch.float64)\n",
      "tensor(0.3286, dtype=torch.float64)\n",
      "tensor(9.8591, dtype=torch.float64)\n",
      "tensor(9.8591, dtype=torch.float64)\n",
      "tensor(0.3286, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "## kl_divのlossの算出方法 -> reductionで設定\n",
    "## reduction = 'elementwise_mean'\n",
    "qlogq = (bn * torch.log(bn)).mean()\n",
    "qlogp = (bn * torch.log(an)).mean()\n",
    "print(qlogq - qlogp)\n",
    "\n",
    "## reduction = 'sum' >> default\n",
    "qlogq = (bn * torch.log(bn)).sum()\n",
    "qlogp = (bn * torch.log(an)).sum()\n",
    "print(qlogq - qlogp)\n",
    "\n",
    "print(F.kl_div(torch.log(an), bn, reduction='elementwise_mean'))\n",
    "print(F.kl_div(torch.log(an), bn, reduction='sum'))\n",
    "print(F.kl_div(torch.log(an), bn, size_average=False))\n",
    "print(F.kl_div(torch.log(an), bn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, logger):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    log = 'Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, total, 100. * correct / total)\n",
    "    logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick_data = []\n",
    "def train(model, device, label_loader, unlabel_loader, testloader,criterion, vat, optimizer, epoch, logger, args):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "#     for batch_id, (data, target) in enumerate(train_loader):\n",
    "#     label_iter = iter(label_loader)\n",
    "#     unlabel_iter = iter(unlabel_loader)\n",
    "    \n",
    "    \n",
    "    for iter_id in range(args.iters):\n",
    "\n",
    "        label_data, label_target = next(label_loader)\n",
    "        unlabel_data, _ = next(unlabel_loader)\n",
    "    \n",
    "#         label_data, label_target = label_iter.next()\n",
    "#         unlabel_data, _ = unlabel_iter.next()\n",
    "            \n",
    "    \n",
    "        label_data_size = label_data.size()[0]\n",
    "        unlabel_data_size = unlabel_data.size()[0]\n",
    "        \n",
    "        \n",
    "        label_data, label_target = label_data.to(device), label_target.to(device)\n",
    "        unlabel_data = unlabel_data.to(device)       \n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        label_output = model(label_data)\n",
    "       \n",
    "        label_loss = criterion(label_output, label_target)\n",
    "#         lds_unlabel = vat(model, unlabel_data)\n",
    "#         lds_label = vat(model, label_data)\n",
    "        lds_unlabel = vat(model, unlabel_data) / unlabel_data_size\n",
    "#         lds_label = vat(model, label_data) / label_data_size\n",
    "        \n",
    "        loss = label_loss + args.alpha * (lds_unlabel)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter_id % args.log_interval == 0:\n",
    "            _, predicted = torch.max(label_output, 1)\n",
    "            total += label_target.size(0)\n",
    "            correct += (predicted == label_target).sum().item() \n",
    "            log = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {:.0f}%'.format(\n",
    "                epoch,\n",
    "                iter_id * len(label_data), args.iters * len(label_data),\n",
    "                100. * (iter_id * len(label_data)) /(args.iters * len(label_data)),\n",
    "                label_loss.item(),\n",
    "                100. * correct / total)\n",
    "            logger.write(log)\n",
    "        \n",
    "        if iter_id % (args.iters * 0.1) == 0:\n",
    "            test(model, device, testloader, criterion, logger)\n",
    "            model.train()\n",
    "#             checkpoint(net, optimizer, epoch, logger, out_dir)\n",
    "#             print('loss:        {}'.format(label_loss))\n",
    "#             print('vat_loss_l:  {}'.format(lds_unlabel))\n",
    "#             print('vat_loss_nl: {}'.format(lds_label))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = easydict.EasyDict({\n",
    "        \"label_batch_size\": 100,\n",
    "        \"unlabel_batch_size\": 200,\n",
    "        \"test_batch_size\": 10,\n",
    "        \"epochs\": 1,\n",
    "        \"iters\": 20000,\n",
    "        \"lr\": 0.01,\n",
    "        \"momentum\": 0.9,\n",
    "        \"alpha\": 1.0,\n",
    "#         \"xi\": 1e-6,\n",
    "        \"xi\": 25,\n",
    "        \"eps\": 2.0,\n",
    "        \"ip\": 1,\n",
    "        \"workers\": 8,\n",
    "        \"seed\": 123,\n",
    "        \"log_interval\": 100,\n",
    "        \"gpu\": 6\n",
    "    })\n",
    "    \n",
    "   \n",
    "    no_cuda = False\n",
    "    out_dir = './result'\n",
    "    train_label_csv = '../data/mnist/train_label.csv'\n",
    "    train_unlabel_csv = '../data/mnist/train_big.csv'\n",
    "    test_csv = '../data/mnist/test_small.csv'\n",
    "    train_root_dir = '../data/mnist/train'\n",
    "    test_root_dir = '../data/mnist/test'\n",
    "    test_interval = 1\n",
    "    resume_interval = 1\n",
    "    \n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()    \n",
    "    torch.manual_seed(args.seed)\n",
    "    device = torch.device('cuda:{}'.format(args.gpu) if use_cuda else 'cpu')\n",
    "    print(device)\n",
    "    kwargs = {'num_workers':8, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))])\n",
    "    \n",
    "    trainset_label = MNISTDataSet(train_label_csv, train_root_dir, transform)    \n",
    "    trainloader_label = torch.utils.data.DataLoader(\n",
    "        trainset_label, batch_size=args.label_batch_size,\n",
    "        sampler=InfiniteSampler(len(trainset_label)),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    trainset_unlabel = MNISTDataSet(train_unlabel_csv, train_root_dir, transform)    \n",
    "    trainloader_unlabel = torch.utils.data.DataLoader(\n",
    "        trainset_unlabel, batch_size=args.unlabel_batch_size,\n",
    "        sampler=InfiniteSampler(len(trainset_unlabel)),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    testset = MNISTDataSet(test_csv, test_root_dir, transform)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=args.test_batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    net = Net2().to(device)\n",
    "#     net = CNN().to(device)\n",
    "#     optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=0.25)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    vat = VAT(args.xi, args.eps, args.ip)\n",
    "    logger = TrainLogger(out_dir)\n",
    "    \n",
    "    data_iterators = get_iters(\n",
    "        root_path='.',\n",
    "        l_batch_size=args.label_batch_size,\n",
    "        ul_batch_size=args.unlabel_batch_size,\n",
    "        test_batch_size=args.test_batch_size,\n",
    "        workers=args.workers\n",
    "    )\n",
    "    \n",
    "    trainloader_label = data_iterators['labeled']\n",
    "    trainloader_unlabel = data_iterators['unlabeled']\n",
    "    testloader = data_iterators['test']\n",
    "    \n",
    "#     for epoch in range(1, args.epochs + 1):\n",
    "    epoch = 1\n",
    "    train(net, device, trainloader_label, trainloader_unlabel, testloader,criterion, vat, optimizer, epoch, logger, args)\n",
    "    test(net, device, testloader, criterion, logger)\n",
    "    \n",
    "#         if epoch % test_interval == 0:\n",
    "#             test(net, device, testloader, criterion, logger)\n",
    "#         if epoch % resume_interval == 0:\n",
    "#             checkpoint(net, optimizer, epoch, logger, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtest\u001b[0m/         test_small.csv  train_big.csv    train_small.csv\r\n",
      "test_big.csv  \u001b[01;34mtrain\u001b[0m/          train_label.csv  train_unlabel.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../../pytorch/data/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../../pytorch/data/mnist/train_big.csv')\n",
    "df_test = pd.read_csv('../../pytorch/data/mnist/test_big.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0/0_00000.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0/0_00001.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0/0_00002.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0/0_00003.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0/0_00004.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img  label\n",
       "0  0/0_00000.png      0\n",
       "1  0/0_00001.png      0\n",
       "2  0/0_00002.png      0\n",
       "3  0/0_00003.png      0\n",
       "4  0/0_00004.png      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labeled = 100\n",
    "valid_size = 1000\n",
    "x_train, y_train = np.array(df_train['img']), np.array(df_train['label'])\n",
    "x_test, y_test =  np.array(df_test['img']), np.array(df_test['label'])\n",
    "\n",
    "randperm = np.random.permutation(len(x_train))\n",
    "labeled_idx = randperm[:n_labeled]\n",
    "validation_idx = randperm[n_labeled:n_labeled + valid_size]\n",
    "unlabeled_idx = randperm[n_labeled + valid_size:]\n",
    "\n",
    "x_labeled = x_train[labeled_idx]\n",
    "x_validation = x_train[validation_idx]\n",
    "x_unlabeled = x_train[unlabeled_idx]\n",
    "\n",
    "y_labeled = y_train[labeled_idx]\n",
    "y_validation = y_train[validation_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../pytorch/data/mnist/train'\n",
    "img_path = os.path.join(path, x_labeled[0])\n",
    "image = Image.open(img_path)\n",
    "# image = image.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.array(image)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "class SimpleDataset2(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform,root):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root, self.x[index])\n",
    "        img = np.array(Image.open(img_path))\n",
    "        img = torch.Tensor(img)\n",
    "        img = img.view([1,28,28])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        target = self.y[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "class InfiniteSampler(sampler.Sampler):\n",
    "\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            order = np.random.permutation(self.num_samples)\n",
    "            for i in range(self.num_samples):\n",
    "                yield order[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_iters2(\n",
    "        dataset='mnist', root_path='.', data_transforms=None,\n",
    "        n_labeled=100, valid_size=1000,\n",
    "        l_batch_size=32, ul_batch_size=128, test_batch_size=256,\n",
    "        workers=8, pseudo_label=None):\n",
    "    \n",
    "    # train_path = f'{root_path}/data/{dataset}/train/'\n",
    "    # test_path = f'{root_path}/data/{dataset}/test/'\n",
    "    \n",
    "    train_root = '../../pytorch/data/mnist/train'\n",
    "    test_root = '../../pytorch/data/mnist/test'\n",
    "    df_train = pd.read_csv('../../pytorch/data/mnist/train_big.csv')\n",
    "    df_test = pd.read_csv('../../pytorch/data/mnist/test_big.csv')\n",
    "    \n",
    "  \n",
    "    if data_transforms is None:\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                # transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,))\n",
    "                # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]),\n",
    "            'eval': transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,))\n",
    "                # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]),\n",
    "        }\n",
    "\n",
    "    # print(train_dataset.train_data)\n",
    "\n",
    "    x_train, y_train = np.array(df_train['img']), np.array(df_train['label'])\n",
    "    x_test, y_test =  np.array(df_test['img']), np.array(df_test['label'])\n",
    "\n",
    "    randperm = np.random.permutation(len(x_train))\n",
    "    labeled_idx = randperm[:n_labeled]\n",
    "    validation_idx = randperm[n_labeled:n_labeled + valid_size]\n",
    "    unlabeled_idx = randperm[n_labeled + valid_size:]\n",
    "\n",
    "    x_labeled = x_train[labeled_idx]\n",
    "    x_validation = x_train[validation_idx]\n",
    "    x_unlabeled = x_train[unlabeled_idx]\n",
    "\n",
    "    y_labeled = y_train[labeled_idx]\n",
    "    y_validation = y_train[validation_idx]\n",
    "    if pseudo_label is None:\n",
    "        y_unlabeled = y_train[unlabeled_idx]\n",
    "    else:\n",
    "        assert isinstance(pseudo_label, np.ndarray)\n",
    "        y_unlabeled = pseudo_label\n",
    "\n",
    "\n",
    "    data_iterators = {\n",
    "        'labeled': iter(DataLoader(\n",
    "            SimpleDataset2(x_labeled, y_labeled, data_transforms['train'], train_root),\n",
    "            batch_size=l_batch_size, num_workers=workers,\n",
    "            sampler=InfiniteSampler(len(x_labeled)),\n",
    "        )),\n",
    "        'unlabeled': iter(DataLoader(\n",
    "            SimpleDataset2(x_unlabeled, y_unlabeled, data_transforms['train'], train_root),\n",
    "            batch_size=ul_batch_size, num_workers=workers,\n",
    "            sampler=InfiniteSampler(len(x_unlabeled)),\n",
    "        )),\n",
    "        'make_pl': iter(DataLoader(\n",
    "            SimpleDataset2(x_unlabeled, y_unlabeled, data_transforms['eval'], train_root),\n",
    "            batch_size=ul_batch_size, num_workers=workers, shuffle=False\n",
    "        )),\n",
    "        'val': iter(DataLoader(\n",
    "            SimpleDataset2(x_validation, y_validation, data_transforms['eval'], train_root),\n",
    "            batch_size=len(x_validation), num_workers=workers, shuffle=False\n",
    "        )),\n",
    "        'test': DataLoader(\n",
    "            SimpleDataset2(x_test, y_test, data_transforms['eval'], test_root),\n",
    "            batch_size=test_batch_size, num_workers=workers, shuffle=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return data_iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, y, transform):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.x[index]\n",
    "        img = img.view([1,28,28])\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        target = self.y[index]\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "class InfiniteSampler(sampler.Sampler):\n",
    "\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            order = np.random.permutation(self.num_samples)\n",
    "            for i in range(self.num_samples):\n",
    "                yield order[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_iters(\n",
    "        dataset='mnist', root_path='.', data_transforms=None,\n",
    "        n_labeled=100, valid_size=1000,\n",
    "        l_batch_size=32, ul_batch_size=128, test_batch_size=256,\n",
    "        workers=8, pseudo_label=None):\n",
    "    \n",
    "    # train_path = f'{root_path}/data/{dataset}/train/'\n",
    "    # test_path = f'{root_path}/data/{dataset}/test/'\n",
    "\n",
    "    train_path = '../../pytorch/data/'\n",
    "    test_path = '../../pytorch/data/'\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "    if dataset == 'CIFAR10':\n",
    "        train_dataset = datasets.CIFAR10(train_path, download=True, train=True, transform=None)\n",
    "        test_dataset = datasets.CIFAR10(test_path, download=True, train=False, transform=None)\n",
    "    elif dataset == 'CIFAR100':\n",
    "        train_dataset = datasets.CIFAR100(train_path, download=True, train=True, transform=None)\n",
    "        test_dataset = datasets.CIFAR100(test_path, download=True, train=False, transform=None)\n",
    "    elif dataset == 'mnist':\n",
    "        print('mnist')\n",
    "        train_dataset = datasets.MNIST(root=train_path, train=True, download=True,transform=transform)\n",
    "        test_dataset = datasets.MNIST(root=test_path, train=False, download=False,transform=transform)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    if data_transforms is None:\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                # transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,))\n",
    "                # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]),\n",
    "            'eval': transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5,), (0.5,))\n",
    "                # transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "            ]),\n",
    "        }\n",
    "\n",
    "    # print(train_dataset.train_data)\n",
    "\n",
    "    x_train, y_train = train_dataset.train_data, np.array(train_dataset.train_labels)\n",
    "    x_test, y_test = test_dataset.test_data, np.array(test_dataset.test_labels)\n",
    "\n",
    "    randperm = np.random.permutation(len(x_train))\n",
    "    labeled_idx = randperm[:n_labeled]\n",
    "    validation_idx = randperm[n_labeled:n_labeled + valid_size]\n",
    "    unlabeled_idx = randperm[n_labeled + valid_size:]\n",
    "\n",
    "    x_labeled = x_train[labeled_idx]\n",
    "    x_validation = x_train[validation_idx]\n",
    "    x_unlabeled = x_train[unlabeled_idx]\n",
    "\n",
    "    y_labeled = y_train[labeled_idx]\n",
    "    y_validation = y_train[validation_idx]\n",
    "    if pseudo_label is None:\n",
    "        y_unlabeled = y_train[unlabeled_idx]\n",
    "    else:\n",
    "        assert isinstance(pseudo_label, np.ndarray)\n",
    "        y_unlabeled = pseudo_label\n",
    "\n",
    "\n",
    "    data_iterators = {\n",
    "        'labeled': iter(DataLoader(\n",
    "            SimpleDataset(x_labeled, y_labeled, data_transforms['train']),\n",
    "            batch_size=l_batch_size, num_workers=workers,\n",
    "            sampler=InfiniteSampler(len(x_labeled)),\n",
    "        )),\n",
    "        'unlabeled': iter(DataLoader(\n",
    "            SimpleDataset(x_unlabeled, y_unlabeled, data_transforms['train']),\n",
    "            batch_size=ul_batch_size, num_workers=workers,\n",
    "            sampler=InfiniteSampler(len(x_unlabeled)),\n",
    "        )),\n",
    "        'make_pl': iter(DataLoader(\n",
    "            SimpleDataset(x_unlabeled, y_unlabeled, data_transforms['eval']),\n",
    "            batch_size=ul_batch_size, num_workers=workers, shuffle=False\n",
    "        )),\n",
    "        'val': iter(DataLoader(\n",
    "            SimpleDataset(x_validation, y_validation, data_transforms['eval']),\n",
    "            batch_size=len(x_validation), num_workers=workers, shuffle=False\n",
    "        )),\n",
    "        'test': DataLoader(\n",
    "            SimpleDataset(x_test, y_test, data_transforms['eval']),\n",
    "            batch_size=test_batch_size, num_workers=workers, shuffle=False\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return data_iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:6\n",
      "mnist\n",
      "Train Epoch: 1 [0/2000000 (0%)]\tLoss: 2.302507\tAccuracy: 10%\n",
      "Test set: Average loss: 2233.5180, Accuracy: 4853/10000 (49%)\n",
      "Train Epoch: 1 [10000/2000000 (0%)]\tLoss: 0.021697\tAccuracy: 55%\n",
      "Train Epoch: 1 [20000/2000000 (1%)]\tLoss: 0.011272\tAccuracy: 70%\n",
      "Train Epoch: 1 [30000/2000000 (2%)]\tLoss: 0.008089\tAccuracy: 78%\n",
      "Train Epoch: 1 [40000/2000000 (2%)]\tLoss: 0.005414\tAccuracy: 82%\n",
      "Train Epoch: 1 [50000/2000000 (2%)]\tLoss: 0.005644\tAccuracy: 85%\n",
      "Train Epoch: 1 [60000/2000000 (3%)]\tLoss: 0.004548\tAccuracy: 87%\n",
      "Train Epoch: 1 [70000/2000000 (4%)]\tLoss: 0.003724\tAccuracy: 89%\n",
      "Train Epoch: 1 [80000/2000000 (4%)]\tLoss: 0.003088\tAccuracy: 90%\n",
      "Train Epoch: 1 [90000/2000000 (4%)]\tLoss: 0.002744\tAccuracy: 91%\n",
      "Train Epoch: 1 [100000/2000000 (5%)]\tLoss: 0.002144\tAccuracy: 92%\n",
      "Train Epoch: 1 [110000/2000000 (6%)]\tLoss: 0.001905\tAccuracy: 92%\n",
      "Train Epoch: 1 [120000/2000000 (6%)]\tLoss: 0.001986\tAccuracy: 93%\n",
      "Train Epoch: 1 [130000/2000000 (6%)]\tLoss: 0.001763\tAccuracy: 94%\n",
      "Train Epoch: 1 [140000/2000000 (7%)]\tLoss: 0.001658\tAccuracy: 94%\n",
      "Train Epoch: 1 [150000/2000000 (8%)]\tLoss: 0.001643\tAccuracy: 94%\n",
      "Train Epoch: 1 [160000/2000000 (8%)]\tLoss: 0.001999\tAccuracy: 95%\n",
      "Train Epoch: 1 [170000/2000000 (8%)]\tLoss: 0.001396\tAccuracy: 95%\n",
      "Train Epoch: 1 [180000/2000000 (9%)]\tLoss: 0.001519\tAccuracy: 95%\n",
      "Train Epoch: 1 [190000/2000000 (10%)]\tLoss: 0.001193\tAccuracy: 96%\n",
      "Train Epoch: 1 [200000/2000000 (10%)]\tLoss: 0.001203\tAccuracy: 96%\n",
      "Test set: Average loss: 92.0467, Accuracy: 9747/10000 (97%)\n",
      "Train Epoch: 1 [210000/2000000 (10%)]\tLoss: 0.001143\tAccuracy: 96%\n",
      "Train Epoch: 1 [220000/2000000 (11%)]\tLoss: 0.001311\tAccuracy: 96%\n",
      "Train Epoch: 1 [230000/2000000 (12%)]\tLoss: 0.001144\tAccuracy: 96%\n",
      "Train Epoch: 1 [240000/2000000 (12%)]\tLoss: 0.001036\tAccuracy: 96%\n",
      "Train Epoch: 1 [250000/2000000 (12%)]\tLoss: 0.001019\tAccuracy: 97%\n",
      "Train Epoch: 1 [260000/2000000 (13%)]\tLoss: 0.000905\tAccuracy: 97%\n",
      "Train Epoch: 1 [270000/2000000 (14%)]\tLoss: 0.001123\tAccuracy: 97%\n",
      "Train Epoch: 1 [280000/2000000 (14%)]\tLoss: 0.000887\tAccuracy: 97%\n",
      "Train Epoch: 1 [290000/2000000 (14%)]\tLoss: 0.001250\tAccuracy: 97%\n",
      "Train Epoch: 1 [300000/2000000 (15%)]\tLoss: 0.000892\tAccuracy: 97%\n",
      "Train Epoch: 1 [310000/2000000 (16%)]\tLoss: 0.001147\tAccuracy: 97%\n",
      "Train Epoch: 1 [320000/2000000 (16%)]\tLoss: 0.000940\tAccuracy: 97%\n",
      "Train Epoch: 1 [330000/2000000 (16%)]\tLoss: 0.000871\tAccuracy: 97%\n",
      "Train Epoch: 1 [340000/2000000 (17%)]\tLoss: 0.000638\tAccuracy: 97%\n",
      "Train Epoch: 1 [350000/2000000 (18%)]\tLoss: 0.000636\tAccuracy: 98%\n",
      "Train Epoch: 1 [360000/2000000 (18%)]\tLoss: 0.000664\tAccuracy: 98%\n",
      "Train Epoch: 1 [370000/2000000 (18%)]\tLoss: 0.000716\tAccuracy: 98%\n",
      "Train Epoch: 1 [380000/2000000 (19%)]\tLoss: 0.000700\tAccuracy: 98%\n",
      "Train Epoch: 1 [390000/2000000 (20%)]\tLoss: 0.000734\tAccuracy: 98%\n",
      "Train Epoch: 1 [400000/2000000 (20%)]\tLoss: 0.000626\tAccuracy: 98%\n",
      "Test set: Average loss: 70.8550, Accuracy: 9785/10000 (98%)\n",
      "Train Epoch: 1 [410000/2000000 (20%)]\tLoss: 0.000717\tAccuracy: 98%\n",
      "Train Epoch: 1 [420000/2000000 (21%)]\tLoss: 0.000628\tAccuracy: 98%\n",
      "Train Epoch: 1 [430000/2000000 (22%)]\tLoss: 0.000678\tAccuracy: 98%\n",
      "Train Epoch: 1 [440000/2000000 (22%)]\tLoss: 0.000686\tAccuracy: 98%\n",
      "Train Epoch: 1 [450000/2000000 (22%)]\tLoss: 0.000556\tAccuracy: 98%\n",
      "Train Epoch: 1 [460000/2000000 (23%)]\tLoss: 0.000473\tAccuracy: 98%\n",
      "Train Epoch: 1 [470000/2000000 (24%)]\tLoss: 0.000782\tAccuracy: 98%\n",
      "Train Epoch: 1 [480000/2000000 (24%)]\tLoss: 0.000402\tAccuracy: 98%\n",
      "Train Epoch: 1 [490000/2000000 (24%)]\tLoss: 0.000498\tAccuracy: 98%\n",
      "Train Epoch: 1 [500000/2000000 (25%)]\tLoss: 0.000619\tAccuracy: 98%\n",
      "Train Epoch: 1 [510000/2000000 (26%)]\tLoss: 0.000546\tAccuracy: 98%\n",
      "Train Epoch: 1 [520000/2000000 (26%)]\tLoss: 0.000522\tAccuracy: 98%\n",
      "Train Epoch: 1 [530000/2000000 (26%)]\tLoss: 0.000504\tAccuracy: 98%\n",
      "Train Epoch: 1 [540000/2000000 (27%)]\tLoss: 0.000495\tAccuracy: 98%\n",
      "Train Epoch: 1 [550000/2000000 (28%)]\tLoss: 0.000493\tAccuracy: 98%\n",
      "Train Epoch: 1 [560000/2000000 (28%)]\tLoss: 0.000512\tAccuracy: 98%\n",
      "Train Epoch: 1 [570000/2000000 (28%)]\tLoss: 0.000473\tAccuracy: 98%\n",
      "Train Epoch: 1 [580000/2000000 (29%)]\tLoss: 0.000524\tAccuracy: 98%\n",
      "Train Epoch: 1 [590000/2000000 (30%)]\tLoss: 0.000478\tAccuracy: 98%\n",
      "Train Epoch: 1 [600000/2000000 (30%)]\tLoss: 0.000401\tAccuracy: 99%\n",
      "Test set: Average loss: 64.2411, Accuracy: 9807/10000 (98%)\n",
      "Train Epoch: 1 [610000/2000000 (30%)]\tLoss: 0.000413\tAccuracy: 99%\n",
      "Train Epoch: 1 [620000/2000000 (31%)]\tLoss: 0.000481\tAccuracy: 99%\n",
      "Train Epoch: 1 [630000/2000000 (32%)]\tLoss: 0.000402\tAccuracy: 99%\n",
      "Train Epoch: 1 [640000/2000000 (32%)]\tLoss: 0.000351\tAccuracy: 99%\n",
      "Train Epoch: 1 [650000/2000000 (32%)]\tLoss: 0.000411\tAccuracy: 99%\n",
      "Train Epoch: 1 [660000/2000000 (33%)]\tLoss: 0.000382\tAccuracy: 99%\n",
      "Train Epoch: 1 [670000/2000000 (34%)]\tLoss: 0.000345\tAccuracy: 99%\n",
      "Train Epoch: 1 [680000/2000000 (34%)]\tLoss: 0.000373\tAccuracy: 99%\n",
      "Train Epoch: 1 [690000/2000000 (34%)]\tLoss: 0.000434\tAccuracy: 99%\n",
      "Train Epoch: 1 [700000/2000000 (35%)]\tLoss: 0.000373\tAccuracy: 99%\n",
      "Train Epoch: 1 [710000/2000000 (36%)]\tLoss: 0.000321\tAccuracy: 99%\n",
      "Train Epoch: 1 [720000/2000000 (36%)]\tLoss: 0.000392\tAccuracy: 99%\n",
      "Train Epoch: 1 [730000/2000000 (36%)]\tLoss: 0.000386\tAccuracy: 99%\n",
      "Train Epoch: 1 [740000/2000000 (37%)]\tLoss: 0.000461\tAccuracy: 99%\n",
      "Train Epoch: 1 [750000/2000000 (38%)]\tLoss: 0.000348\tAccuracy: 99%\n",
      "Train Epoch: 1 [760000/2000000 (38%)]\tLoss: 0.000381\tAccuracy: 99%\n",
      "Train Epoch: 1 [770000/2000000 (38%)]\tLoss: 0.000290\tAccuracy: 99%\n",
      "Train Epoch: 1 [780000/2000000 (39%)]\tLoss: 0.000364\tAccuracy: 99%\n",
      "Train Epoch: 1 [790000/2000000 (40%)]\tLoss: 0.000365\tAccuracy: 99%\n",
      "Train Epoch: 1 [800000/2000000 (40%)]\tLoss: 0.000312\tAccuracy: 99%\n",
      "Test set: Average loss: 57.6284, Accuracy: 9830/10000 (98%)\n",
      "Train Epoch: 1 [810000/2000000 (40%)]\tLoss: 0.000342\tAccuracy: 99%\n",
      "Train Epoch: 1 [820000/2000000 (41%)]\tLoss: 0.000375\tAccuracy: 99%\n",
      "Train Epoch: 1 [830000/2000000 (42%)]\tLoss: 0.000339\tAccuracy: 99%\n",
      "Train Epoch: 1 [840000/2000000 (42%)]\tLoss: 0.000239\tAccuracy: 99%\n",
      "Train Epoch: 1 [850000/2000000 (42%)]\tLoss: 0.000273\tAccuracy: 99%\n",
      "Train Epoch: 1 [860000/2000000 (43%)]\tLoss: 0.000275\tAccuracy: 99%\n",
      "Train Epoch: 1 [870000/2000000 (44%)]\tLoss: 0.000238\tAccuracy: 99%\n",
      "Train Epoch: 1 [880000/2000000 (44%)]\tLoss: 0.000310\tAccuracy: 99%\n",
      "Train Epoch: 1 [890000/2000000 (44%)]\tLoss: 0.000249\tAccuracy: 99%\n",
      "Train Epoch: 1 [900000/2000000 (45%)]\tLoss: 0.000313\tAccuracy: 99%\n",
      "Train Epoch: 1 [910000/2000000 (46%)]\tLoss: 0.000327\tAccuracy: 99%\n",
      "Train Epoch: 1 [920000/2000000 (46%)]\tLoss: 0.000282\tAccuracy: 99%\n",
      "Train Epoch: 1 [930000/2000000 (46%)]\tLoss: 0.000246\tAccuracy: 99%\n",
      "Train Epoch: 1 [940000/2000000 (47%)]\tLoss: 0.000331\tAccuracy: 99%\n",
      "Train Epoch: 1 [950000/2000000 (48%)]\tLoss: 0.000238\tAccuracy: 99%\n",
      "Train Epoch: 1 [960000/2000000 (48%)]\tLoss: 0.000281\tAccuracy: 99%\n",
      "Train Epoch: 1 [970000/2000000 (48%)]\tLoss: 0.000280\tAccuracy: 99%\n",
      "Train Epoch: 1 [980000/2000000 (49%)]\tLoss: 0.000223\tAccuracy: 99%\n",
      "Train Epoch: 1 [990000/2000000 (50%)]\tLoss: 0.000212\tAccuracy: 99%\n",
      "Train Epoch: 1 [1000000/2000000 (50%)]\tLoss: 0.000346\tAccuracy: 99%\n",
      "Test set: Average loss: 62.5707, Accuracy: 9814/10000 (98%)\n",
      "Train Epoch: 1 [1010000/2000000 (50%)]\tLoss: 0.000207\tAccuracy: 99%\n",
      "Train Epoch: 1 [1020000/2000000 (51%)]\tLoss: 0.000243\tAccuracy: 99%\n",
      "Train Epoch: 1 [1030000/2000000 (52%)]\tLoss: 0.000257\tAccuracy: 99%\n",
      "Train Epoch: 1 [1040000/2000000 (52%)]\tLoss: 0.000227\tAccuracy: 99%\n",
      "Train Epoch: 1 [1050000/2000000 (52%)]\tLoss: 0.000244\tAccuracy: 99%\n",
      "Train Epoch: 1 [1060000/2000000 (53%)]\tLoss: 0.000306\tAccuracy: 99%\n",
      "Train Epoch: 1 [1070000/2000000 (54%)]\tLoss: 0.000198\tAccuracy: 99%\n",
      "Train Epoch: 1 [1080000/2000000 (54%)]\tLoss: 0.000293\tAccuracy: 99%\n",
      "Train Epoch: 1 [1090000/2000000 (54%)]\tLoss: 0.000284\tAccuracy: 99%\n",
      "Train Epoch: 1 [1100000/2000000 (55%)]\tLoss: 0.000199\tAccuracy: 99%\n",
      "Train Epoch: 1 [1110000/2000000 (56%)]\tLoss: 0.000270\tAccuracy: 99%\n",
      "Train Epoch: 1 [1120000/2000000 (56%)]\tLoss: 0.000262\tAccuracy: 99%\n",
      "Train Epoch: 1 [1130000/2000000 (56%)]\tLoss: 0.000230\tAccuracy: 99%\n",
      "Train Epoch: 1 [1140000/2000000 (57%)]\tLoss: 0.000227\tAccuracy: 99%\n",
      "Train Epoch: 1 [1150000/2000000 (58%)]\tLoss: 0.000211\tAccuracy: 99%\n",
      "Train Epoch: 1 [1160000/2000000 (58%)]\tLoss: 0.000184\tAccuracy: 99%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [1170000/2000000 (58%)]\tLoss: 0.000206\tAccuracy: 99%\n",
      "Train Epoch: 1 [1180000/2000000 (59%)]\tLoss: 0.000205\tAccuracy: 99%\n",
      "Train Epoch: 1 [1190000/2000000 (60%)]\tLoss: 0.000193\tAccuracy: 99%\n",
      "Train Epoch: 1 [1200000/2000000 (60%)]\tLoss: 0.000288\tAccuracy: 99%\n",
      "Test set: Average loss: 53.8434, Accuracy: 9835/10000 (98%)\n",
      "Train Epoch: 1 [1210000/2000000 (60%)]\tLoss: 0.000183\tAccuracy: 99%\n",
      "Train Epoch: 1 [1220000/2000000 (61%)]\tLoss: 0.000190\tAccuracy: 99%\n",
      "Train Epoch: 1 [1230000/2000000 (62%)]\tLoss: 0.000192\tAccuracy: 99%\n",
      "Train Epoch: 1 [1240000/2000000 (62%)]\tLoss: 0.000200\tAccuracy: 99%\n",
      "Train Epoch: 1 [1250000/2000000 (62%)]\tLoss: 0.000204\tAccuracy: 99%\n",
      "Train Epoch: 1 [1260000/2000000 (63%)]\tLoss: 0.000199\tAccuracy: 99%\n",
      "Train Epoch: 1 [1270000/2000000 (64%)]\tLoss: 0.000168\tAccuracy: 99%\n",
      "Train Epoch: 1 [1280000/2000000 (64%)]\tLoss: 0.000169\tAccuracy: 99%\n",
      "Train Epoch: 1 [1290000/2000000 (64%)]\tLoss: 0.000196\tAccuracy: 99%\n",
      "Train Epoch: 1 [1300000/2000000 (65%)]\tLoss: 0.000201\tAccuracy: 99%\n",
      "Train Epoch: 1 [1310000/2000000 (66%)]\tLoss: 0.000171\tAccuracy: 99%\n",
      "Train Epoch: 1 [1320000/2000000 (66%)]\tLoss: 0.000292\tAccuracy: 99%\n",
      "Train Epoch: 1 [1330000/2000000 (66%)]\tLoss: 0.000152\tAccuracy: 99%\n",
      "Train Epoch: 1 [1340000/2000000 (67%)]\tLoss: 0.000180\tAccuracy: 99%\n",
      "Train Epoch: 1 [1350000/2000000 (68%)]\tLoss: 0.000155\tAccuracy: 99%\n",
      "Train Epoch: 1 [1360000/2000000 (68%)]\tLoss: 0.000215\tAccuracy: 99%\n",
      "Train Epoch: 1 [1370000/2000000 (68%)]\tLoss: 0.000179\tAccuracy: 99%\n",
      "Train Epoch: 1 [1380000/2000000 (69%)]\tLoss: 0.000238\tAccuracy: 99%\n",
      "Train Epoch: 1 [1390000/2000000 (70%)]\tLoss: 0.000166\tAccuracy: 99%\n",
      "Train Epoch: 1 [1400000/2000000 (70%)]\tLoss: 0.000146\tAccuracy: 99%\n",
      "Test set: Average loss: 54.7581, Accuracy: 9841/10000 (98%)\n",
      "Train Epoch: 1 [1410000/2000000 (70%)]\tLoss: 0.000193\tAccuracy: 99%\n",
      "Train Epoch: 1 [1420000/2000000 (71%)]\tLoss: 0.000196\tAccuracy: 99%\n",
      "Train Epoch: 1 [1430000/2000000 (72%)]\tLoss: 0.000147\tAccuracy: 99%\n",
      "Train Epoch: 1 [1440000/2000000 (72%)]\tLoss: 0.000196\tAccuracy: 99%\n",
      "Train Epoch: 1 [1450000/2000000 (72%)]\tLoss: 0.000130\tAccuracy: 99%\n",
      "Train Epoch: 1 [1460000/2000000 (73%)]\tLoss: 0.000171\tAccuracy: 99%\n",
      "Train Epoch: 1 [1470000/2000000 (74%)]\tLoss: 0.000142\tAccuracy: 99%\n",
      "Train Epoch: 1 [1480000/2000000 (74%)]\tLoss: 0.000168\tAccuracy: 99%\n",
      "Train Epoch: 1 [1490000/2000000 (74%)]\tLoss: 0.000155\tAccuracy: 99%\n",
      "Train Epoch: 1 [1500000/2000000 (75%)]\tLoss: 0.000158\tAccuracy: 99%\n",
      "Train Epoch: 1 [1510000/2000000 (76%)]\tLoss: 0.000157\tAccuracy: 99%\n",
      "Train Epoch: 1 [1520000/2000000 (76%)]\tLoss: 0.000132\tAccuracy: 99%\n",
      "Train Epoch: 1 [1530000/2000000 (76%)]\tLoss: 0.000158\tAccuracy: 99%\n",
      "Train Epoch: 1 [1540000/2000000 (77%)]\tLoss: 0.000163\tAccuracy: 99%\n",
      "Train Epoch: 1 [1550000/2000000 (78%)]\tLoss: 0.000150\tAccuracy: 99%\n",
      "Train Epoch: 1 [1560000/2000000 (78%)]\tLoss: 0.000162\tAccuracy: 99%\n",
      "Train Epoch: 1 [1570000/2000000 (78%)]\tLoss: 0.000142\tAccuracy: 99%\n",
      "Train Epoch: 1 [1580000/2000000 (79%)]\tLoss: 0.000142\tAccuracy: 99%\n",
      "Train Epoch: 1 [1590000/2000000 (80%)]\tLoss: 0.000114\tAccuracy: 99%\n",
      "Train Epoch: 1 [1600000/2000000 (80%)]\tLoss: 0.000186\tAccuracy: 99%\n",
      "Test set: Average loss: 52.2111, Accuracy: 9847/10000 (98%)\n",
      "Train Epoch: 1 [1610000/2000000 (80%)]\tLoss: 0.000140\tAccuracy: 99%\n",
      "Train Epoch: 1 [1620000/2000000 (81%)]\tLoss: 0.000138\tAccuracy: 99%\n",
      "Train Epoch: 1 [1630000/2000000 (82%)]\tLoss: 0.000140\tAccuracy: 99%\n",
      "Train Epoch: 1 [1640000/2000000 (82%)]\tLoss: 0.000160\tAccuracy: 99%\n",
      "Train Epoch: 1 [1650000/2000000 (82%)]\tLoss: 0.000149\tAccuracy: 99%\n",
      "Train Epoch: 1 [1660000/2000000 (83%)]\tLoss: 0.000153\tAccuracy: 99%\n",
      "Train Epoch: 1 [1670000/2000000 (84%)]\tLoss: 0.000143\tAccuracy: 99%\n",
      "Train Epoch: 1 [1680000/2000000 (84%)]\tLoss: 0.000130\tAccuracy: 99%\n",
      "Train Epoch: 1 [1690000/2000000 (84%)]\tLoss: 0.000137\tAccuracy: 99%\n",
      "Train Epoch: 1 [1700000/2000000 (85%)]\tLoss: 0.000124\tAccuracy: 99%\n",
      "Train Epoch: 1 [1710000/2000000 (86%)]\tLoss: 0.000107\tAccuracy: 99%\n",
      "Train Epoch: 1 [1720000/2000000 (86%)]\tLoss: 0.000124\tAccuracy: 99%\n",
      "Train Epoch: 1 [1730000/2000000 (86%)]\tLoss: 0.000138\tAccuracy: 99%\n",
      "Train Epoch: 1 [1740000/2000000 (87%)]\tLoss: 0.000140\tAccuracy: 99%\n",
      "Train Epoch: 1 [1750000/2000000 (88%)]\tLoss: 0.000118\tAccuracy: 99%\n",
      "Train Epoch: 1 [1760000/2000000 (88%)]\tLoss: 0.000122\tAccuracy: 99%\n",
      "Train Epoch: 1 [1770000/2000000 (88%)]\tLoss: 0.000177\tAccuracy: 99%\n",
      "Train Epoch: 1 [1780000/2000000 (89%)]\tLoss: 0.000110\tAccuracy: 99%\n",
      "Train Epoch: 1 [1790000/2000000 (90%)]\tLoss: 0.000137\tAccuracy: 100%\n",
      "Train Epoch: 1 [1800000/2000000 (90%)]\tLoss: 0.000129\tAccuracy: 100%\n",
      "Test set: Average loss: 55.6129, Accuracy: 9841/10000 (98%)\n",
      "Train Epoch: 1 [1810000/2000000 (90%)]\tLoss: 0.000120\tAccuracy: 100%\n",
      "Train Epoch: 1 [1820000/2000000 (91%)]\tLoss: 0.000103\tAccuracy: 100%\n",
      "Train Epoch: 1 [1830000/2000000 (92%)]\tLoss: 0.000096\tAccuracy: 100%\n",
      "Train Epoch: 1 [1840000/2000000 (92%)]\tLoss: 0.000111\tAccuracy: 100%\n",
      "Train Epoch: 1 [1850000/2000000 (92%)]\tLoss: 0.000118\tAccuracy: 100%\n",
      "Train Epoch: 1 [1860000/2000000 (93%)]\tLoss: 0.000103\tAccuracy: 100%\n",
      "Train Epoch: 1 [1870000/2000000 (94%)]\tLoss: 0.000119\tAccuracy: 100%\n",
      "Train Epoch: 1 [1880000/2000000 (94%)]\tLoss: 0.000108\tAccuracy: 100%\n",
      "Train Epoch: 1 [1890000/2000000 (94%)]\tLoss: 0.000090\tAccuracy: 100%\n",
      "Train Epoch: 1 [1900000/2000000 (95%)]\tLoss: 0.000092\tAccuracy: 100%\n",
      "Train Epoch: 1 [1910000/2000000 (96%)]\tLoss: 0.000106\tAccuracy: 100%\n",
      "Train Epoch: 1 [1920000/2000000 (96%)]\tLoss: 0.000096\tAccuracy: 100%\n",
      "Train Epoch: 1 [1930000/2000000 (96%)]\tLoss: 0.000112\tAccuracy: 100%\n",
      "Train Epoch: 1 [1940000/2000000 (97%)]\tLoss: 0.000143\tAccuracy: 100%\n",
      "Train Epoch: 1 [1950000/2000000 (98%)]\tLoss: 0.000128\tAccuracy: 100%\n",
      "Train Epoch: 1 [1960000/2000000 (98%)]\tLoss: 0.000116\tAccuracy: 100%\n",
      "Train Epoch: 1 [1970000/2000000 (98%)]\tLoss: 0.000130\tAccuracy: 100%\n",
      "Train Epoch: 1 [1980000/2000000 (99%)]\tLoss: 0.000073\tAccuracy: 100%\n",
      "Train Epoch: 1 [1990000/2000000 (100%)]\tLoss: 0.000089\tAccuracy: 100%\n",
      "Test set: Average loss: 52.5532, Accuracy: 9847/10000 (98%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2aeaa7b44898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 345, in get\n",
      "    return ForkingPickler.loads(res)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 181, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/usr/lib/python3.5/multiprocessing/reduction.py\", line 152, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtest\u001b[0m/         test_small.csv  train_big.csv    train_small.csv\r\n",
      "test_big.csv  \u001b[01;34mtrain\u001b[0m/          train_label.csv  train_unlabel.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
