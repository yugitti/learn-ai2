{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import easydict\n",
    "\n",
    "%matplotlib inline\n",
    "custom_style = {'axes.labelcolor': 'white',\n",
    "                'xtick.color': 'white',\n",
    "                'ytick.color': 'white'}\n",
    "sns.set_style(\"darkgrid\", rc=custom_style)\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"font.size\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PytorchでMNISTを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考サイト\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--label_batch_size', type=int, default=32, metavar='N',\n",
    "#                         help='input batch size for training (default: 32)')\n",
    "#     parser.add_argument('--unlabel_batch_size', type=int, default=128, metavar='N',\n",
    "#                         help='input batch size for training (default: 32)')\n",
    "#     parser.add_argument('--test_batch_size', type=int, default=1000, metavar='N',\n",
    "#                         help='input batch size for testing (default: 1000)')\n",
    "#     parser.add_argument('--epoch', type=int, default=10, metavar='N',\n",
    "#                         help='number of iterations to train (default: 10)')\n",
    "#     parser.add_argument('--iters', type=int, default=400, metavar='N',\n",
    "#                         help='number of iterations to train (default: 10000)')\n",
    "#     parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                         help='learning rate (default: 0.01)')\n",
    "#     parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "#                         help='SGD momentum (default: 0.9)')\n",
    "#     parser.add_argument('--alpha', type=float, default=1.0, metavar='ALPHA',\n",
    "#                         help='regularization coefficient (default: 0.01)')\n",
    "#     parser.add_argument('--xi', type=float, default=10.0, metavar='XI',\n",
    "#                         help='hyperparameter of VAT (default: 0.1)')\n",
    "#     parser.add_argument('--eps', type=float, default=1.0, metavar='EPS',\n",
    "#                         help='hyperparameter of VAT (default: 1.0)')\n",
    "#     parser.add_argument('--ip', type=int, default=1, metavar='IP',\n",
    "#                         help='hyperparameter of VAT (default: 1)')\n",
    "#     parser.add_argument('--workers', type=int, default=8, metavar='W',\n",
    "#                         help='number of CPU')\n",
    "#     parser.add_argument('--seed', type=int, default=123, metavar='S',\n",
    "#                         help='random seed (default: 1)')\n",
    "#     parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "#                         help='how many batches to wait before logging training status')\n",
    "#     parser.add_argument('--gpu', type=int, default=1, metavar='W',\n",
    "#                         help='number of CPU')\n",
    "#     args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #畳み込み層を定義する\n",
    "        #引数は順番に、サンプル数、チャネル数、フィルタのサイズ\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        #フィルタのサイズは正方形であればタプルではなく整数でも可（8行目と10行目は同じ意味）\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        #全結合層を定義する\n",
    "        #fc1の第一引数は、チャネル数*最後のプーリング層の出力のマップのサイズ=特徴量の数\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(500, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #入力→畳み込み層1→活性化関数(ReLU)→プーリング層1(2*2)→出力\n",
    "        # input 28 x 28 x 1\n",
    "        # conv1 28 x 28 x 1 -> 24 x 24 x 10\n",
    "        # max_pool(kernel2) 12 x 12 x 10\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2,2) )\n",
    "        \n",
    "        #入力→畳み込み層2→活性化関数(ReLU)→プーリング層2(2*2)→出力\n",
    "        # conv2 12 x 12 x 10 -> 8 x 8 x 20\n",
    "        # max_pool(kernel2) -> 4 x 4 x 20\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv2_drop(x)\n",
    "        # output layer\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        # self.num_flat_featuresで特徴量の数を算出\n",
    "        # flatten 4 x 4 x 20 = 320\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "#         x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        #Conv2dは入力を4階のテンソルとして保持する(サンプル数*チャネル数*縦の長さ*横の長さ)\n",
    "        #よって、特徴量の数を数える時は[1:]でスライスしたものを用いる\n",
    "        size = x.size()[1:] ## all dimensions except the batch dimension\n",
    "        #特徴量の数=チャネル数*縦の長さ*横の長さを計算する\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger(object):\n",
    "    \n",
    "    def __init__(self, out):\n",
    "        try:\n",
    "            os.makedirs(out)\n",
    "        except OSError:\n",
    "            pass\n",
    "        self.file = open(os.path.join(out, 'log'), 'w')\n",
    "        self.logs = []\n",
    "        \n",
    "    def write(self, log):\n",
    "        ## write log\n",
    "        tqdm.write(log)\n",
    "        tqdm.write(log, file=self.file)\n",
    "        self.logs.append(log)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        ## returns the state of the loggers\n",
    "        return {'logs': self.logs}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        ## load the logger state\n",
    "        self.logs = state_dict['logs']\n",
    "        #write logs\n",
    "        tqdm.write(self.logs[-1])\n",
    "        for log in self.logs:\n",
    "            tqdm.write(log, file=self.file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(net, optimizer, epoch, logger, out):\n",
    "    filename = os.path.join(out, 'epoch-{}'.format(epoch))\n",
    "    torch.save({'epoch': epoch + 1, 'logger': logger.state_dict()}, filename + '.iter')\n",
    "    torch.save(net.state_dict(), filename + 'model')\n",
    "    torch.save(optimizer.state_dict(), filename + 'state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, logger):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    log = '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, total, 100. * correct / total)\n",
    "    logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteSampler(sampler.Sampler):\n",
    "\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            order = np.random.permutation(self.num_samples)\n",
    "            for i in range(self.num_samples):\n",
    "                yield order[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataSet(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "\n",
    "        self.image_dataframe = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.image_dataframe.loc[idx, 'img'])\n",
    "#         image = io.imread(img_name)\n",
    "        image = Image.open(img_name)\n",
    "        image = image.convert('L')\n",
    "        label = self.image_dataframe.loc[idx, 'label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAT(nn.Module):\n",
    "    def __init__(self, xi=10.0, eps=1.0, ip=1):\n",
    "        \n",
    "        super(VAT, self).__init__()\n",
    "        self.xi = xi\n",
    "        self.eps = eps\n",
    "        self.ip = ip\n",
    "    \n",
    "    def forward(self, model, x):\n",
    "        \n",
    "        ## ラベル無しデータをネットワークに通し、predictを得る\n",
    "        ## VATのLossのbackpropagationでは、このmodel計算は含めないため\n",
    "        ## no_gradでwrapする\n",
    "        with torch.no_grad():\n",
    "            out = model(x)\n",
    "            pred = F.cross_entropy(out, dim=1)\n",
    "        \n",
    "        ## 累積法を用いてVadvを計算する\n",
    "        \n",
    "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, label_loader, unlabel_loader, criterion, optimizer, epoch, logger, args):\n",
    "    model.train()\n",
    "#     for batch_id, (data, target) in enumerate(train_loader):\n",
    "    label_iter = iter(label_loader)\n",
    "    unlabel_iter = iter(unlabel_loader)\n",
    "    \n",
    "    for iter_id in range(args.iters):\n",
    "\n",
    "        label_data, label_target = label_iter.next()\n",
    "        unlabel_data, _ = unlabel_iter.next()\n",
    "        \n",
    "        label_data, label_target = label_data.to(device), label_target.to(device)\n",
    "        unlabel_data = unlabel_data.to(device)       \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        label_output = model(label_data)\n",
    "        label_loss = criterion(label_output, label_target)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        label_loss.backward()\n",
    "        optimizer.step()\n",
    "        if iter_id % args.log_interval == 0:\n",
    "            log = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                iter_id * len(label_data), args.iters * len(label_data),\n",
    "                100. * (iter_id * len(label_data)) /(args.iters * len(label_data)),\n",
    "                label_loss.item())\n",
    "            logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = easydict.EasyDict({\n",
    "        \"label_batch_size\": 32,\n",
    "        \"unlabel_batch_size\": 100,\n",
    "        \"test_batch_size\": 1000,\n",
    "        \"epochs\": 2,\n",
    "        \"iters\": 400,\n",
    "        \"lr\": 0.01,\n",
    "        \"momentum\": 0.9,\n",
    "        \"alpha\": 1.0,\n",
    "        \"xi\": 10.0,\n",
    "        \"eps\": 1.0,\n",
    "        \"ip\": 1,\n",
    "        \"workers\": 8,\n",
    "        \"seed\": 123,\n",
    "        \"log_interval\": 100,\n",
    "        \"gpu\": 1\n",
    "    })\n",
    "    \n",
    "   \n",
    "    no_cuda = False\n",
    "    out_dir = './result'\n",
    "    train_label_csv = '../data/mnist/train_label.csv'\n",
    "    train_unlabel_csv = '../data/mnist/train_unlabel.csv'\n",
    "    test_csv = '../data/mnist/test_big.csv'\n",
    "    train_root_dir = '../data/mnist/train'\n",
    "    test_root_dir = '../data/mnist/test'\n",
    "    test_interval = 1\n",
    "    resume_interval = 1\n",
    "    \n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()    \n",
    "    torch.manual_seed(args.seed)\n",
    "    device = torch.device('cuda:{}'.format(args.gpu) if use_cuda else 'cpu')\n",
    "    print(device)\n",
    "    kwargs = {'num_workers':8, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))])\n",
    "    \n",
    "    trainset_label = MNISTDataSet(train_label_csv, train_root_dir, transform)    \n",
    "    trainloader_label = torch.utils.data.DataLoader(\n",
    "        trainset_label, batch_size=args.label_batch_size, shuffle=False,\n",
    "        sampler=InfiniteSampler(len(trainset_label)),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    trainset_unlabel = MNISTDataSet(train_unlabel_csv, train_root_dir, transform)    \n",
    "    trainloader_unlabel = torch.utils.data.DataLoader(\n",
    "        trainset_unlabel, batch_size=args.unlabel_batch_size, shuffle=False,\n",
    "        sampler=InfiniteSampler(len(trainset_unlabel)),\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    testset = MNISTDataSet(test_csv, test_root_dir, transform)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=args.test_batch_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logger = TrainLogger(out_dir)\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(net, device, trainloader_label, trainloader_unlabel, criterion, optimizer, epoch, logger, args)\n",
    "        if epoch % test_interval == 0:\n",
    "            test(net, device, testloader, criterion, logger)\n",
    "        if epoch % resume_interval == 0:\n",
    "            checkpoint(net, optimizer, epoch, logger, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Train Epoch: 1 [0/12800 (0%)]\tLoss: 2.300159\n",
      "Train Epoch: 1 [3200/12800 (25%)]\tLoss: 0.921981\n",
      "Train Epoch: 1 [6400/12800 (50%)]\tLoss: 0.065908\n",
      "Train Epoch: 1 [9600/12800 (75%)]\tLoss: 0.030152\n",
      "\n",
      "Test set: Average loss: 18.7518, Accuracy: 7535/10000 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/12800 (0%)]\tLoss: 0.029724\n",
      "Train Epoch: 2 [3200/12800 (25%)]\tLoss: 0.008896\n",
      "Train Epoch: 2 [6400/12800 (50%)]\tLoss: 0.008138\n",
      "Train Epoch: 2 [9600/12800 (75%)]\tLoss: 0.051199\n",
      "\n",
      "Test set: Average loss: 20.5748, Accuracy: 7636/10000 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtest\u001b[0m/         test_small.csv  train_big.csv    train_small.csv\r\n",
      "test_big.csv  \u001b[01;34mtrain\u001b[0m/          train_label.csv  train_unlabel.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls ../data/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
