{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "%matplotlib inline\n",
    "custom_style = {'axes.labelcolor': 'white',\n",
    "                'xtick.color': 'white',\n",
    "                'ytick.color': 'white'}\n",
    "sns.set_style(\"darkgrid\", rc=custom_style)\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams[\"font.size\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PytorchでMNISTを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参考サイト\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    " \n",
    "def progress(p, l, e):\n",
    "    sys.stdout.write(\"\\repoch [{}]\\t{} / 100\".format(e, int(p * 100 / (l - 1))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #畳み込み層を定義する\n",
    "        #引数は順番に、サンプル数、チャネル数、フィルタのサイズ\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        #フィルタのサイズは正方形であればタプルではなく整数でも可（8行目と10行目は同じ意味）\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        #全結合層を定義する\n",
    "        #fc1の第一引数は、チャネル数*最後のプーリング層の出力のマップのサイズ=特徴量の数\n",
    "        \n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #入力→畳み込み層1→活性化関数(ReLU)→プーリング層1(2*2)→出力\n",
    "        # input 28 x 28 x 1\n",
    "        # conv1 28 x 28 x 1 -> 24 x 24 x 10\n",
    "        # max_pool(kernel2) 12 x 12 x 10\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2,2) )\n",
    "        \n",
    "        #入力→畳み込み層2→活性化関数(ReLU)→プーリング層2(2*2)→出力\n",
    "        # conv2 12 x 12 x 10 -> 8 x 8 x 20\n",
    "        # max_pool(kernel2) -> 4 x 4 x 20\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv2_drop(x)\n",
    "        # output layer\n",
    "        #x = x.view(-1, self.num_flat_features(x))\n",
    "        # self.num_flat_featuresで特徴量の数を算出\n",
    "        # flatten 4 x 4 x 20 = 320\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        #Conv2dは入力を4階のテンソルとして保持する(サンプル数*チャネル数*縦の長さ*横の長さ)\n",
    "        #よって、特徴量の数を数える時は[1:]でスライスしたものを用いる\n",
    "        size = x.size()[1:] ## all dimensions except the batch dimension\n",
    "        #特徴量の数=チャネル数*縦の長さ*横の長さを計算する\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLogger(object):\n",
    "    \n",
    "    def __init__(self, out):\n",
    "        try:\n",
    "            os.makedirs(out)\n",
    "        except OSError:\n",
    "            pass\n",
    "        self.file = open(os.path.join(out, 'log'), 'w')\n",
    "        self.logs = []\n",
    "        \n",
    "    def write(self, log):\n",
    "        ## write log\n",
    "#         tqdm.write(log)\n",
    "#         tqdm.write(log, file=self.file)\n",
    "        print('{}'.format(log))\n",
    "        self.file.write((log + \"\\n\"))\n",
    "#         print(log, file=self.file)\n",
    "        self.logs.append(log)\n",
    "        \n",
    "    def state_dict(self):\n",
    "        ## returns the state of the loggers\n",
    "        return {'logs': self.logs}\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        ## load the logger state\n",
    "        self.logs = state_dict['logs']\n",
    "        #write logs\n",
    "        tqdm.write(self.logs[-1])\n",
    "        for log in self.logs:\n",
    "            tqdm.write(log, file=self.file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(net, optimizer, epoch, logger, out):\n",
    "    filename = os.path.join(out, 'epoch-{}'.format(epoch))\n",
    "    torch.save({'epoch': epoch + 1, 'logger': logger.state_dict()}, filename + '.iter')\n",
    "    torch.save(net.state_dict(), filename + 'model')\n",
    "    torch.save(optimizer.state_dict(), filename + 'state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch, log_interval, logger):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_id, (data, target) in enumerate(train_loader, start=1):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "        if batch_id % log_interval == 0:\n",
    "#             log = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_id * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_id / len(train_loader), loss.item())\n",
    "\n",
    "            progress(batch_id, len(train_loader), epoch)\n",
    "#             print(len(train_loader))\n",
    "#             logger.write(log)\n",
    "    print(' ')\n",
    "#     log = 'EPOCH [{}]: Train Loss: {:.6f}'.format(epoch, loss.item())\n",
    "    log = 'EPOCH [{}]: Train set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "    epoch, loss, correct, total, 100. * correct / total)\n",
    "#     progress(batch_id, len(train_loader))\n",
    "#     print('\\n')\n",
    "    logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, logger):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    log = 'Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, total, 100. * correct / total)\n",
    "    logger.write(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    batch_size = 100\n",
    "    test_bach_size = 100\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.5\n",
    "    no_cuda = False\n",
    "    seed = 123\n",
    "    log_interval = 10\n",
    "    out_dir = './result'\n",
    "    test_interval = 1\n",
    "    resume_interval = 1\n",
    "    \n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()    \n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, ), (0.5,))])\n",
    "    \n",
    "    trainset = datasets.MNIST(\n",
    "        root = './data', train=True, download=True,transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    )\n",
    "    testset = datasets.MNIST(\n",
    "        root = './data', train=False, download=True,transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=test_bach_size, shuffle=False, **kwargs\n",
    "    )\n",
    "    \n",
    "    net = Net().to(device)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    logger = TrainLogger(out_dir)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(net, device, trainloader, criterion, optimizer, epoch, log_interval, logger)\n",
    "        if epoch % test_interval == 0:\n",
    "            test(net, device, testloader, criterion, logger)\n",
    "        if epoch % resume_interval == 0:\n",
    "            checkpoint(net, optimizer, epoch, logger, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "epoch [1]\t100 / 100 \n",
      "EPOCH [1]: Train set: Average loss: 0.6258, Accuracy: 23524/60000 (39%)\n",
      "Test set: Average loss: 44.2229, Accuracy: 8715/10000 (87%)\n",
      "epoch [2]\t100 / 100 \n",
      "EPOCH [2]: Train set: Average loss: 0.3338, Accuracy: 53484/60000 (89%)\n",
      "Test set: Average loss: 18.9096, Accuracy: 9437/10000 (94%)\n",
      "epoch [3]\t100 / 100 \n",
      "EPOCH [3]: Train set: Average loss: 0.1905, Accuracy: 56035/60000 (93%)\n",
      "Test set: Average loss: 12.3017, Accuracy: 9640/10000 (96%)\n",
      "epoch [4]\t100 / 100 \n",
      "EPOCH [4]: Train set: Average loss: 0.1454, Accuracy: 56857/60000 (95%)\n",
      "Test set: Average loss: 9.8470, Accuracy: 9698/10000 (97%)\n",
      "epoch [5]\t100 / 100 \n",
      "EPOCH [5]: Train set: Average loss: 0.2857, Accuracy: 57286/60000 (95%)\n",
      "Test set: Average loss: 7.8616, Accuracy: 9753/10000 (98%)\n",
      "epoch [6]\t100 / 100 \n",
      "EPOCH [6]: Train set: Average loss: 0.0925, Accuracy: 57624/60000 (96%)\n",
      "Test set: Average loss: 7.1455, Accuracy: 9770/10000 (98%)\n",
      "epoch [7]\t100 / 100 \n",
      "EPOCH [7]: Train set: Average loss: 0.0694, Accuracy: 57830/60000 (96%)\n",
      "Test set: Average loss: 6.4227, Accuracy: 9789/10000 (98%)\n",
      "epoch [8]\t100 / 100 \n",
      "EPOCH [8]: Train set: Average loss: 0.2034, Accuracy: 58025/60000 (97%)\n",
      "Test set: Average loss: 5.5455, Accuracy: 9823/10000 (98%)\n",
      "epoch [9]\t100 / 100 \n",
      "EPOCH [9]: Train set: Average loss: 0.0937, Accuracy: 58130/60000 (97%)\n",
      "Test set: Average loss: 5.0241, Accuracy: 9831/10000 (98%)\n",
      "epoch [10]\t100 / 100 \n",
      "EPOCH [10]: Train set: Average loss: 0.1280, Accuracy: 58282/60000 (97%)\n",
      "Test set: Average loss: 4.9737, Accuracy: 9832/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
